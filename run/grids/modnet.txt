# only switch the data that is loaded

# The GNN approach optimises and outputs soft modularity whereas the greedy optimisation
# works with crisp modularity. Given crisp assignments, the soft modularity is equal to
# the crisp modularity.

# Format for each row: name in config.py; alias; range to search
# No spaces, except between these 3 fields

# Line breaks are used to union different grid search spaces
# Feel free to add '#' to add comments

# 0.) ground-truth and baseline
train.mode mode ['groundtruth','run_alg']

# 1.) "GENERAL" hParameters
# Motivation: 
# - lrs of 0.01 commonly seen in publications,
#   0.1 seemingly worked well in common experiments,
#   extremely large lrs seemingly worked better for real-world networks
# - collapse regularisation: freestyle, since we use a different term
# - dropout: as commonly seen
# - features: one-hot encodings as baseline
# (repeat: 5)
# dataset.augment_feature feat [['node_onehot_full']]
# optim.base_lr lr [0.01,0.1,0.5]
# model.collapse_lam lam [0.0,0.1,0.25,0.5]
# gnn.dropout dropout [0.0,0.2,0.4]

# 2.) Batch Normalisation
# picked best model from above
# (repeat: 25)
# dataset.augment_feature feat [['node_onehot_full']]
# optim.base_lr lr [0.01]
# model.collapse_lam lam [0.5]
# gnn.dropout dropout [0.2]
# gnn.batchnorm batchnorm [True,False]

# inconclusive from running only on 5, lets try 25...
# slightly better results, lower std variation

# 3.) Features
# (repeat: 15)
# dataset.augment_feature feat [['node_onehot_full'],['node_bethe_hessian'],['node_laplacian']]
# optim.base_lr lr [0.01]
# model.collapse_lam lam [0.5]
# gnn.dropout dropout [0.2]
# gnn.batchnorm batchnorm [True]

# !! 1Hot seems to achieve slightly better modularity than BH but higher variation
# Laplacian does not work at all

# 4.) Number and size of layers
# Motivation: ModNet has hidden_dim of 48, layers_post 1
# DMoN has hidden_dim of 64, layers_post 0
# (repeat: 15)
# dataset.augment_feature feat [['node_onehot_full'],['node_bethe_hessian']]
# optim.base_lr lr [0.01]
# model.collapse_lam lam [0.5]
# gnn.dropout dropout [0.2]
# gnn.batchnorm batchnorm [True]
# gnn.dim_inner dim_inner [16,64,128]
# gnn.layers_mp layers_mp [1,2,4]
# gnn.layers_post_mp layers_post [0,1,2]

# takeaway 4 or 2 message-passing layers always performed best

# 6.) Favourite model with more epochs
# (repeat: 5)
# dataset.augment_feature feat [['node_bethe_hessian']]
# optim.base_lr lr [0.01]
# model.collapse_lam lam [0.5]
# gnn.dropout dropout [0.2]
# gnn.batchnorm batchnorm [True]
# gnn.dim_inner dim_inner [128]
# gnn.layers_mp layers_mp [4]
# gnn.layers_post_mp layers_post [2]
# optim.max_epoch epoch [800]

# brought a slight improvement in performance.
# tuning this will probably also be linked to settings of learn rate decay.

# note: no attention mechanism like in modnet
# no skip connections


# ----

# another well-performing model
# modnetBase-batchnorm=True-dim_inner=64-dropout=0.2-lam=0.25-layers_mp=4-layers_post=2
# gnn.batchnorm batchnorm [True]
# gnn.dim_inner dim_inner [64]
# gnn.dropout dropout [0.2]
# model.collapse_lam lam [0.25]
# gnn.layers_mp layers_mp [4]
# gnn.layers_post_mp layers_post [2]
# dataset.augment_feature feat [['node_bethe_hessian']]
