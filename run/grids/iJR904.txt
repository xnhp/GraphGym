# baseline
train.mode mode ['run_alg']

# yaml base config file based on favourite model for modnet
# re-evaluate lambda since we no longer can expect evenly sized communities
# dataset.augment_feature augment [[],['node_onehot_full'],['node_bethe_hessian']]
# dataset.use_node_feature use_feat [True,False]
# model.collapse_lam lam [0.0,0.2,0.7]




# other experiments:
# seeing that community sizes are relatively even and
# we get a top-performing modularity with lam=0.2, lets try lam=0.0
# dim_inner = 128 picked since that was best performing model with low lambda
# also experiment with learn rate because why not
# dataset.augment_feature feat [['node_onehot_full']]
# optim.base_lr lr [0.1,0.3,0.5,1.2]
# gnn.batchnorm batchnorm [True]
# gnn.dim_inner dim_inner [128]
# gnn.dropout dropout [0.2]
# model.collapse_lam lam [0.0]
# gnn.layers_pre_mp layers_pre [0]
# gnn.layers_mp layers_mp [4]
# gnn.layers_post_mp layers_post [2]

# dataset.use_node_feature [True,False]
# optim.base_lr lr [0.1]
# gnn.batchnorm batchnorm [True]
# gnn.dim_inner dim_inner [64,128]
# gnn.dropout dropout [0.2]
# model.collapse_lam lam [0.2,0.5,0.7,1.0]
# gnn.layers_pre_mp layers_pre [0]
# gnn.layers_mp layers_mp [4]
# gnn.layers_post_mp layers_post [2]


